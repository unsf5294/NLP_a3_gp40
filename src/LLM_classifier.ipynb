{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d113cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "906aed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train-claims.json\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"../data/dev-claims.json\") as f:\n",
    "    dev = json.load(f)\n",
    "with open(\"../data/evidence.json\") as f:\n",
    "    evidence = json.load(f)\n",
    "with open(\"../data/test-evidence-faiss.json\") as f:\n",
    "    test_faiss = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba00bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fewshot_prompt():\n",
    "\n",
    "    FEWSHOT = \"\"\"You are a fact verification assistant.\n",
    "Your task is to choose the correct classification label for a given claim based on the supporting evidence statements.\n",
    "Possible labels:\n",
    "- SUPPORTS: All evidence supports the claim as factual.\n",
    "- REFUTES: All evidence contradicts the claim.\n",
    "- NOT_ENOUGH_INFO: The evidence is insufficient, irrelevant, or off-topic.\n",
    "- DISPUTED: Evidence is conflicting or controversial.\n",
    "\n",
    "Output only the label.\"\"\"\n",
    "\n",
    "    label_buckets = {\"SUPPORTS\": [], \"REFUTES\": [], \"NOT_ENOUGH_INFO\": [], \"DISPUTED\": []}\n",
    "    for cid, obj in train.items():\n",
    "        label = obj[\"claim_label\"]\n",
    "        if label in label_buckets and len(label_buckets[label]) < 3:\n",
    "            sents = [evidence[eid] for eid in obj[\"evidences\"] if eid in evidence]\n",
    "            if len(sents) >= 1:\n",
    "                label_buckets[label].append((obj[\"claim_text\"], sents[:2]))\n",
    "\n",
    "    # Flatten all examples into one string\n",
    "    for label, examples in label_buckets.items():\n",
    "        for claim, sents in examples:\n",
    "            FEWSHOT += f\"\\n\\n### EXAMPLE\\nClaim: {claim}\\nStatements:\"\n",
    "            for s in sents:\n",
    "                FEWSHOT += f\"\\n- \\\"{s}\\\"\"\n",
    "            FEWSHOT += f\"\\nClass: {label}\"\n",
    "\n",
    "    return FEWSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b039b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fewshot_prompt_cot():\n",
    "    FEWSHOT = \"\"\"You are a fact verification assistant.\n",
    "Your task is to choose the correct classification label for a given claim based on the supporting evidence statements.\n",
    "Possible labels:\n",
    "- SUPPORTS: All evidence supports the claim as factual.\n",
    "- REFUTES: All evidence contradicts the claim.\n",
    "- NOT_ENOUGH_INFO: The evidence is insufficient, irrelevant, or off-topic.\n",
    "- DISPUTED: Evidence is conflicting or controversial.\n",
    "\n",
    "Before making a decision, explain your reasoning based on the statements.\n",
    "Output the reasoning followed by the label.\"\"\"\n",
    "\n",
    "    label_buckets = {\"SUPPORTS\": [], \"REFUTES\": [], \"NOT_ENOUGH_INFO\": [], \"DISPUTED\": []}\n",
    "    for cid, obj in train.items():\n",
    "        label = obj[\"claim_label\"]\n",
    "        if label in label_buckets and len(label_buckets[label]) < 2:\n",
    "            sents = [evidence[eid] for eid in obj[\"evidences\"] if eid in evidence]\n",
    "            if len(sents) >= 1:\n",
    "                label_buckets[label].append((obj[\"claim_text\"], sents[:2]))\n",
    "\n",
    "    for label, examples in label_buckets.items():\n",
    "        for claim, sents in examples:\n",
    "            FEWSHOT += f\"\\n\\n### EXAMPLE\\nClaim: {claim}\\nStatements:\"\n",
    "            for s in sents:\n",
    "                FEWSHOT += f\"\\n- \\\"{s}\\\"\"\n",
    "            FEWSHOT += f\"\\nReasoning: [explain how the evidences relates to the claim.]\"  # LLM completes this\n",
    "            FEWSHOT += f\"\\nClass: {label}\"\n",
    "\n",
    "    return FEWSHOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a014d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOT = build_fewshot_prompt() # chain of thought or normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c1b8687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:47<00:00, 15.86s/it]\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Using Mistral-7B\n",
    "MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True\n",
    "    ),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(MODEL),\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.0 # labelling task, avoid random\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b170f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = FEWSHOT + \"\"\"\n",
    "### TASK\n",
    "Claim: {claim}\n",
    "Statements:\n",
    "{evidence_lines}\n",
    "Class:\n",
    "\"\"\"\n",
    "\n",
    "def classify_claim(claim: str, evidence_sents: list[str]) -> str:\n",
    "    ev_lines = \"\\n\".join(f\"- \\\"{s}\\\"\" for s in evidence_sents[:5]) # 5 evidences\n",
    "    prompt   = PROMPT.format(claim=claim, evidence_lines=ev_lines)\n",
    "    out      = llm(prompt)[0][\"generated_text\"].split(\"Class:\")[-1].strip()\n",
    "    match    = re.search(r\"(SUPPORTS|REFUTES|NOT_ENOUGH_INFO|DISPUTED)\", out.upper())\n",
    "    return match.group(1).upper() if match else \"NOT_ENOUGH_INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ccefb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM classify:   0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   1%|          | 1/154 [00:14<37:51, 14.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   1%|▏         | 2/154 [00:37<48:44, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   2%|▏         | 3/154 [00:40<29:54, 11.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   3%|▎         | 4/154 [00:45<23:08,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   3%|▎         | 5/154 [00:49<18:28,  7.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   4%|▍         | 6/154 [00:54<16:23,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   5%|▍         | 7/154 [00:59<14:56,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   5%|▌         | 8/154 [01:03<12:41,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   6%|▌         | 9/154 [01:08<12:31,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   6%|▋         | 10/154 [01:13<12:23,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   7%|▋         | 11/154 [01:18<12:03,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   8%|▊         | 12/154 [01:23<11:58,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   8%|▊         | 13/154 [01:26<10:37,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:   9%|▉         | 14/154 [01:31<11:09,  4.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  10%|▉         | 15/154 [01:36<11:15,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  10%|█         | 16/154 [01:42<11:30,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  11%|█         | 17/154 [01:47<11:14,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  12%|█▏        | 18/154 [01:52<11:14,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  12%|█▏        | 19/154 [01:57<11:14,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  13%|█▎        | 20/154 [02:02<11:08,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  14%|█▎        | 21/154 [02:06<10:55,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  14%|█▍        | 22/154 [02:11<10:48,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  15%|█▍        | 23/154 [02:16<10:29,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  16%|█▌        | 24/154 [02:19<09:25,  4.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  16%|█▌        | 25/154 [02:24<09:43,  4.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  17%|█▋        | 26/154 [02:28<09:31,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  18%|█▊        | 27/154 [02:31<08:32,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  18%|█▊        | 28/154 [02:36<08:56,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  19%|█▉        | 29/154 [02:41<09:26,  4.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  19%|█▉        | 30/154 [02:46<09:32,  4.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  20%|██        | 31/154 [02:50<09:14,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  21%|██        | 32/154 [02:55<09:23,  4.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  21%|██▏       | 33/154 [03:00<09:21,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  22%|██▏       | 34/154 [03:05<09:31,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  23%|██▎       | 35/154 [03:09<09:11,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  23%|██▎       | 36/154 [03:14<09:03,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  24%|██▍       | 37/154 [03:19<09:15,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  25%|██▍       | 38/154 [03:24<09:12,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  25%|██▌       | 39/154 [03:29<09:07,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  26%|██▌       | 40/154 [03:33<09:03,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  27%|██▋       | 41/154 [03:38<09:02,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  27%|██▋       | 42/154 [03:43<09:10,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  28%|██▊       | 43/154 [03:48<09:04,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  29%|██▊       | 44/154 [04:00<12:28,  6.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  29%|██▉       | 45/154 [04:06<12:01,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  30%|██▉       | 46/154 [04:14<12:41,  7.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  31%|███       | 47/154 [04:24<14:26,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  31%|███       | 48/154 [04:33<14:28,  8.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  32%|███▏      | 49/154 [04:39<13:13,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  32%|███▏      | 50/154 [04:51<15:44,  9.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  33%|███▎      | 51/154 [04:57<13:41,  7.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  34%|███▍      | 52/154 [05:02<11:58,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  34%|███▍      | 53/154 [05:13<14:12,  8.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  35%|███▌      | 54/154 [05:49<27:34, 16.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  36%|███▌      | 55/154 [06:32<40:21, 24.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  36%|███▋      | 56/154 [07:01<42:12, 25.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  37%|███▋      | 57/154 [08:37<1:15:57, 46.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  38%|███▊      | 58/154 [09:03<1:04:50, 40.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  38%|███▊      | 59/154 [09:08<47:23, 29.93s/it]  Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  39%|███▉      | 60/154 [09:13<35:09, 22.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  40%|███▉      | 61/154 [09:18<26:45, 17.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  40%|████      | 62/154 [09:22<20:27, 13.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "LLM classify:  40%|████      | 62/154 [09:25<13:59,  9.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m claim \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m ev_sents \u001b[38;5;241m=\u001b[39m [evidence[eid] \u001b[38;5;28;01mfor\u001b[39;00m eid \u001b[38;5;129;01min\u001b[39;00m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidences\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m----> 5\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_claim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclaim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mev_sents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m out[cid] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidences\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidences\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m, in \u001b[0;36mclassify_claim\u001b[0;34m(claim, evidence_sents)\u001b[0m\n\u001b[1;32m     10\u001b[0m ev_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m evidence_sents[:\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;66;03m# 5 evidences\u001b[39;00m\n\u001b[1;32m     11\u001b[0m prompt   \u001b[38;5;241m=\u001b[39m PROMPT\u001b[38;5;241m.\u001b[39mformat(claim\u001b[38;5;241m=\u001b[39mclaim, evidence_lines\u001b[38;5;241m=\u001b[39mev_lines)\n\u001b[0;32m---> 12\u001b[0m out      \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     13\u001b[0m match    \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(SUPPORTS|REFUTES|NOT_ENOUGH_INFO|DISPUTED)\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mupper())\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ENOUGH_INFO\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1379\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         )\n\u001b[1;32m   1377\u001b[0m     )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1386\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1385\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1386\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3422\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3420\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3422\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3423\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3424\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3426\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2616\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2616\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "for cid, obj in tqdm(dev.items(), desc=\"LLM classify\"):\n",
    "    claim = obj[\"claim_text\"]\n",
    "    ev_sents = [evidence[eid] for eid in obj[\"evidences\"]]\n",
    "    label = classify_claim(claim, ev_sents)\n",
    "    out[cid] = {\"claim_label\": label, \"evidences\": obj[\"evidences\"]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Label counts:\n",
      "            SUPPORTS: 72\n",
      "     NOT_ENOUGH_INFO: 58\n",
      "            DISPUTED: 17\n",
      "             REFUTES: 7\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter([entry[\"claim_label\"] for entry in out.values()])\n",
    "\n",
    "print(\"🔢 Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label:>20}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f3dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ \n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/dev-predicted-mistral.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "print(\"✓ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43308ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for cid, obj in tqdm(test_faiss.items(), desc=\"LLM classify\"):\n",
    "    claim = obj[\"claim_text\"]\n",
    "    ev_sents = [evidence[eid] for eid in obj[\"evidences\"]]\n",
    "    label = classify_claim(claim, ev_sents)\n",
    "    out[cid] = {\"claim_label\": label, \"evidences\": obj[\"evidences\"]}\n",
    "\n",
    "\n",
    "with open(\"../data/test-predicted-mistral.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "print(\"✓ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter([entry[\"claim_label\"] for entry in out.values()])\n",
    "\n",
    "print(\"🔢 Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label:>20}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
