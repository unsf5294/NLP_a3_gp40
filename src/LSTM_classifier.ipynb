{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "611979d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, ujson as json\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9f7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claim and evidence files\n",
    "with open(\"../data/train-claims.json\") as f:\n",
    "    claim = json.load(f)\n",
    "\n",
    "with open(\"../data/evidence.json\") as f:\n",
    "    evidence = json.load(f)\n",
    "\n",
    "# Build dataframe with evidence texts\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"id\": cid,\n",
    "        \"text\": \"[CLAIM] \" + item[\"claim_text\"] + \" [EVIDENCE] \" + \" \".join([\n",
    "            evidence[eid] for eid in item[\"evidences\"] if eid in evidence\n",
    "        ]),\n",
    "        \"label\": item[\"claim_label\"]\n",
    "    }\n",
    "    for cid, item in claim.items()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc7f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"label_encoded\"] = le.fit_transform(df[\"label\"])\n",
    "NUM_CLASSES = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1017824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'global_max_pooling1d_10' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fold 1 best val_accuracy: 0.5163\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'global_max_pooling1d_11' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fold 2 best val_accuracy: 0.5854\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'global_max_pooling1d_12' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fold 3 best val_accuracy: 0.5650\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'global_max_pooling1d_13' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fold 4 best val_accuracy: 0.5755\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'global_max_pooling1d_14' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fold 5 best val_accuracy: 0.5469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "checkpoint_path = \"../data/LSTM_model.keras\"\n",
    "\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "val_accuracies = []\n",
    "best_model = None\n",
    "best_val_acc = 0\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df[\"text\"], df[\"label_encoded\"])):\n",
    "    print(f\"\\nFold {fold+1}/{k}\")\n",
    "    \n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    # Recreate TextVectorizer (can also cache across folds)\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=10000, output_sequence_length=256, pad_to_max_tokens=True\n",
    "    )\n",
    "    vectorizer.adapt(train_df[\"text\"].values)\n",
    "\n",
    "    # Create datasets\n",
    "    def make_ds(d): return tf.data.Dataset.from_tensor_slices((d[\"text\"].values, d[\"label_encoded\"].values))\\\n",
    "        .batch(64).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = make_ds(train_df)\n",
    "    val_ds = make_ds(val_df)\n",
    "\n",
    "    # Define model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "        vectorizer,\n",
    "        tf.keras.layers.Embedding(10000, 64, mask_zero=True),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(len(df[\"label_encoded\"].unique()), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=10,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_acc = max(history.history[\"val_accuracy\"])\n",
    "    print(f\"âœ… Fold {fold+1} best val_accuracy: {val_acc:.4f}\")\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = model\n",
    "        model.save(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84665926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Cross-validation result:\n",
      "Mean val_accuracy: 0.5578\n",
      "Std dev: 0.0244\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“Š Cross-validation result:\")\n",
    "print(f\"Mean val_accuracy: {np.mean(val_accuracies):.4f}\")\n",
    "print(f\"Std dev: {np.std(val_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b0e7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dev-claims.json\") as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "output = {}\n",
    "\n",
    "for cid, obj in dev.items():\n",
    "    claim_text = obj[\"claim_text\"]\n",
    "    evidence_strs = [evidence.get(eid, \"\") for eid in obj[\"evidences\"][:3]]  # top 3\n",
    "    text_input = \"[CLAIM] \" + claim_text + \" [EVIDENCE] \" + \" \".join(evidence_strs)\n",
    "    \n",
    "    # Predict: you must wrap in np.array and use .reshape or tf.convert\n",
    "    pred = model.predict(tf.convert_to_tensor([text_input]), verbose=0)\n",
    "    label = le.inverse_transform([np.argmax(pred)])[0]\n",
    "    \n",
    "    output[cid] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"evidences\": obj[\"evidences\"],\n",
    "        \"claim_label\": label\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae92081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Label counts:\n",
      "            SUPPORTS: 123\n",
      "     NOT_ENOUGH_INFO: 31\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter([entry[\"claim_label\"] for entry in output.values()])\n",
    "\n",
    "print(\"ðŸ”¢ Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label:>20}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c04a51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dev-predicted-lstm.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4640e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/test-evidence-faiss.json\") as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "output = {}\n",
    "\n",
    "for cid, obj in test.items():\n",
    "    claim_text = obj[\"claim_text\"]\n",
    "    evidence_strs = [evidence.get(eid, \"\") for eid in obj[\"evidences\"][:3]]  # top 3\n",
    "    text_input = \"[CLAIM] \" + claim_text + \" [EVIDENCE] \" + \" \".join(evidence_strs)\n",
    "    \n",
    "    # Predict: you must wrap in np.array and use .reshape or tf.convert\n",
    "    pred = model.predict(tf.convert_to_tensor([text_input]), verbose=0)\n",
    "    label = le.inverse_transform([np.argmax(pred)])[0]\n",
    "    \n",
    "    output[cid] = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"evidences\": obj[\"evidences\"],\n",
    "        \"claim_label\": label\n",
    "    }\n",
    "with open(\"../data/test-predicted-lstm.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     sequence_length = 256\n",
    "#     vocab_size = 10000\n",
    "#     batch_size = 64\n",
    "#     embed_dim = 128\n",
    "#     hidden_1 = 32\n",
    "#     hidden_2 = 32\n",
    "#     lr = 1e-3\n",
    "#     epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = tf.keras.layers.TextVectorization(\n",
    "#     max_tokens=CFG.vocab_size,\n",
    "#     output_sequence_length=CFG.sequence_length,\n",
    "#     pad_to_max_tokens=True\n",
    "# )\n",
    "# vectorizer.adapt(train_df[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(dataframe, shuffle=True):\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((dataframe[\"text\"].values, dataframe[\"label_encoded\"].values))\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(1024)\n",
    "#     return ds.batch(CFG.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# train_ds = create_dataset(train_df)\n",
    "# val_ds = create_dataset(val_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "#     vectorizer,\n",
    "#     tf.keras.layers.Embedding(CFG.vocab_size, CFG.embed_dim, mask_zero=True),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(CFG.hidden_1, return_sequences=True)),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(CFG.hidden_2)),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.lr),\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a20405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = compute_class_weight(\n",
    "#     class_weight='balanced',\n",
    "#     classes=np.unique(train_df[\"label_encoded\"]),\n",
    "#     y=train_df[\"label_encoded\"]\n",
    "# )\n",
    "# class_weights = dict(enumerate(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67289dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train + save \n",
    "# checkpoint_path = \"../data/LSTM_model.keras\"\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=CFG.epochs,\n",
    "#     class_weight=class_weights,\n",
    "#     callbacks=[\n",
    "#         tf.keras.callbacks.ModelCheckpoint(\n",
    "#             checkpoint_path,\n",
    "#             save_best_only=True,\n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         ),\n",
    "#         tf.keras.callbacks.EarlyStopping(\n",
    "#             monitor='val_loss', \n",
    "#             patience=3, \n",
    "#             restore_best_weights=True\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_learning(history):\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     for i, key in enumerate([\"loss\", \"accuracy\"]):\n",
    "#         plt.subplot(1, 2, i+1)\n",
    "#         plt.plot(history.history[key], label=\"train\")\n",
    "#         plt.plot(history.history[f\"val_{key}\"], label=\"val\")\n",
    "#         plt.title(key.capitalize())\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(key)\n",
    "#         plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_learning(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/dev-claims.json\") as f:\n",
    "#     dev = json.load(f)\n",
    "\n",
    "# # Load your trained model\n",
    "# model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# output = {}\n",
    "\n",
    "# for cid, obj in dev.items():\n",
    "#     claim_text = obj[\"claim_text\"]\n",
    "#     evidence_strs = [evidences.get(eid, \"\") for eid in obj[\"evidences\"][:3]]  # top 3\n",
    "#     text_input = \"[CLAIM] \" + claim_text + \" [EVIDENCE] \" + \" \".join(evidence_strs)\n",
    "    \n",
    "#     # Predict: you must wrap in np.array and use .reshape or tf.convert\n",
    "#     pred = model.predict(tf.convert_to_tensor([text_input]), verbose=0)\n",
    "#     label = le.inverse_transform([np.argmax(pred)])[0]\n",
    "    \n",
    "#     output[cid] = {\n",
    "#         \"claim_text\": claim_text,\n",
    "#         \"evidences\": obj[\"evidences\"],\n",
    "#         \"claim_label\": label\n",
    "#     }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
